{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simulation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flwr\n",
        "!pip install 'ray[default]'\n",
        "!pip install aiohttp==3.7.4\n",
        "!pip uninstall aioredis\n",
        "!pip install aioredis==1.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr6SCzDaZksa",
        "outputId": "9d0434c5-c769-426b-d082-8f02521f4f97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flwr\n",
            "  Downloading flwr-0.17.0-py3-none-any.whl (229 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 40 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 51 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 61 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 81 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 92 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 102 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 112 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 122 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 133 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 143 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 153 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 163 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 174 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 184 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 194 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 204 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 215 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 225 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 229 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google<3.0.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from flwr) (2.0.3)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.1 in /usr/local/lib/python3.7/dist-packages (from flwr) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from flwr) (1.19.5)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.27.2 in /usr/local/lib/python3.7/dist-packages (from flwr) (1.43.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from google<3.0.0,>=2.0.3->flwr) (4.6.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2.0.0,>=1.27.2->flwr) (1.15.0)\n",
            "Installing collected packages: flwr\n",
            "Successfully installed flwr-0.17.0\n",
            "Collecting ray[default]\n",
            "  Downloading ray-1.9.2-cp37-cp37m-manylinux2014_x86_64.whl (57.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 57.6 MB 69 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.43.0)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.13)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[default]) (21.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.4.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]) (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.19.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.0.3)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.1.2-py3-none-any.whl (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.11-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 33.6 MB/s \n",
            "\u001b[?25hCollecting opencensus\n",
            "  Downloading opencensus-0.8.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 49.4 MB/s \n",
            "\u001b[?25hCollecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful\n",
            "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[K     |████████████████████████████████| 201 kB 35.4 MB/s \n",
            "\u001b[?25hCollecting frozenlist\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.0 MB/s \n",
            "\u001b[?25hCollecting gpustat>=1.0.0b1\n",
            "  Downloading gpustat-1.0.0b1.tar.gz (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 304 kB/s \n",
            "\u001b[?25hCollecting aiohttp>=3.7\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[default]) (2.23.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.7/dist-packages (from ray[default]) (5.2.1)\n",
            "Collecting aioredis<2\n",
            "  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (0.13.1)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]) (3.10.0.2)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]) (2.0.11)\n",
            "Collecting hiredis\n",
            "  Downloading hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[default]) (1.15.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[default]) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[default]) (5.4.8)\n",
            "Collecting blessed>=1.17.1\n",
            "  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[default]) (0.2.5)\n",
            "Collecting deprecated>=1.2.3\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[default]) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[default]) (4.10.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[default]) (1.13.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[default]) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.0->ray[default]) (3.0.7)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.7->ray[default]) (2.10)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[default]) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[default]) (5.4.0)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]) (1.26.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.35.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.54.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (3.0.4)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.0.0b1-py3-none-any.whl size=15979 sha256=d69434b86979992930c8e12db47fb04752cff7c2b011e5727ae8db7e778d7e1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/16/e2/3e2437fba4c4b6a97a97bd96fce5d14e66cff5c4966fb1cc8c\n",
            "Successfully built gpustat\n",
            "Installing collected packages: multidict, frozenlist, yarl, deprecated, asynctest, async-timeout, aiosignal, redis, opencensus-context, hiredis, blessed, aiohttp, ray, py-spy, opencensus, gpustat, colorful, aioredis, aiohttp-cors\n",
            "Successfully installed aiohttp-3.8.1 aiohttp-cors-0.7.0 aioredis-1.3.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 blessed-1.19.1 colorful-0.5.4 deprecated-1.2.13 frozenlist-1.3.0 gpustat-1.0.0b1 hiredis-2.0.0 multidict-6.0.2 opencensus-0.8.0 opencensus-context-0.1.2 py-spy-0.3.11 ray-1.9.2 redis-4.1.2 yarl-1.7.2\n",
            "Collecting aiohttp==3.7.4\n",
            "  Downloading aiohttp-3.7.4-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4) (21.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4) (6.0.2)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4) (1.7.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.7.4) (2.10)\n",
            "Installing collected packages: async-timeout, aiohttp\n",
            "  Attempting uninstall: async-timeout\n",
            "    Found existing installation: async-timeout 4.0.2\n",
            "    Uninstalling async-timeout-4.0.2:\n",
            "      Successfully uninstalled async-timeout-4.0.2\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.8.1\n",
            "    Uninstalling aiohttp-3.8.1:\n",
            "      Successfully uninstalled aiohttp-3.8.1\n",
            "Successfully installed aiohttp-3.7.4 async-timeout-3.0.1\n",
            "Found existing installation: aioredis 1.3.1\n",
            "Uninstalling aioredis-1.3.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/aioredis-1.3.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/aioredis/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled aioredis-1.3.1\n",
            "Collecting aioredis==1.3.1\n",
            "  Using cached aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.7/dist-packages (from aioredis==1.3.1) (2.0.0)\n",
            "Requirement already satisfied: async-timeout in /usr/local/lib/python3.7/dist-packages (from aioredis==1.3.1) (3.0.1)\n",
            "Installing collected packages: aioredis\n",
            "Successfully installed aioredis-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft28n8PsYdbr",
        "outputId": "dd89957e-b0e7-44d3-ca74-4a7207b9ab2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile dataset_utils.py\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from torchvision.datasets import VisionDataset\n",
        "from typing import Callable, Optional, Tuple, Any\n",
        "from flwr.dataset.utils.common import create_lda_partitions\n",
        "\n",
        "\n",
        "def get_dataset(path_to_data: Path, cid: str, partition: str):\n",
        "\n",
        "    # generate path to cid's data\n",
        "    path_to_data = path_to_data / cid / (partition + \".pt\")\n",
        "\n",
        "    return TorchVision_FL(path_to_data, transform=cifar10Transformation())\n",
        "\n",
        "\n",
        "def get_dataloader(\n",
        "    path_to_data: str, cid: str, is_train: bool, batch_size: int, workers: int\n",
        "):\n",
        "    \"\"\"Generates trainset/valset object and returns appropiate dataloader.\"\"\"\n",
        "\n",
        "    partition = \"train\" if is_train else \"val\"\n",
        "    dataset = get_dataset(Path(path_to_data), cid, partition)\n",
        "\n",
        "    # we use as number of workers all the cpu cores assigned to this actor\n",
        "    kwargs = {\"num_workers\": workers, \"pin_memory\": True, \"drop_last\": False}\n",
        "    return DataLoader(dataset, batch_size=batch_size, **kwargs)\n",
        "\n",
        "\n",
        "def get_random_id_splits(total: int, val_ratio: float, shuffle: bool = True):\n",
        "    \"\"\"splits a list of length `total` into two following a\n",
        "    (1-val_ratio):val_ratio partitioning.\n",
        "\n",
        "    By default the indices are shuffled before creating the split and\n",
        "    returning.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(total, int):\n",
        "        indices = list(range(total))\n",
        "    else:\n",
        "        indices = total\n",
        "\n",
        "    split = int(np.floor(val_ratio * len(indices)))\n",
        "    # print(f\"Users left out for validation (ratio={val_ratio}) = {split} \")\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    return indices[split:], indices[:split]\n",
        "\n",
        "\n",
        "def do_fl_partitioning(path_to_dataset, pool_size, alpha, num_classes, val_ratio=0.0):\n",
        "    \"\"\"Torchvision (e.g. CIFAR-10) datasets using LDA.\"\"\"\n",
        "\n",
        "    images, labels = torch.load(path_to_dataset)\n",
        "    idx = np.array(range(len(images)))\n",
        "    dataset = [idx, labels]\n",
        "    partitions, _ = create_lda_partitions(\n",
        "        dataset, num_partitions=pool_size, concentration=alpha, accept_imbalanced=True\n",
        "    )\n",
        "\n",
        "    # Show label distribution for first partition (purely informative)\n",
        "    partition_zero = partitions[0][1]\n",
        "    hist, _ = np.histogram(partition_zero, bins=list(range(num_classes + 1)))\n",
        "    print(\n",
        "        f\"Class histogram for 0-th partition (alpha={alpha}, {num_classes} classes): {hist}\"\n",
        "    )\n",
        "\n",
        "    # now save partitioned dataset to disk\n",
        "    # first delete dir containing splits (if exists), then create it\n",
        "    splits_dir = path_to_dataset.parent / \"federated\"\n",
        "    if splits_dir.exists():\n",
        "        shutil.rmtree(splits_dir)\n",
        "    Path.mkdir(splits_dir, parents=True)\n",
        "\n",
        "    for p in range(pool_size):\n",
        "\n",
        "        labels = partitions[p][1]\n",
        "        image_idx = partitions[p][0]\n",
        "        imgs = images[image_idx]\n",
        "\n",
        "        # create dir\n",
        "        Path.mkdir(splits_dir / str(p))\n",
        "\n",
        "        if val_ratio > 0.0:\n",
        "            # split data according to val_ratio\n",
        "            train_idx, val_idx = get_random_id_splits(len(labels), val_ratio)\n",
        "            val_imgs = imgs[val_idx]\n",
        "            val_labels = labels[val_idx]\n",
        "\n",
        "            with open(splits_dir / str(p) / \"val.pt\", \"wb\") as f:\n",
        "                torch.save([val_imgs, val_labels], f)\n",
        "\n",
        "            # remaining images for training\n",
        "            imgs = imgs[train_idx]\n",
        "            labels = labels[train_idx]\n",
        "\n",
        "        with open(splits_dir / str(p) / \"train.pt\", \"wb\") as f:\n",
        "            torch.save([imgs, labels], f)\n",
        "\n",
        "    return splits_dir\n",
        "\n",
        "\n",
        "def cifar10Transformation():\n",
        "\n",
        "    return transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "class TorchVision_FL(VisionDataset):\n",
        "    \"\"\"This is just a trimmed down version of torchvision.datasets.MNIST.\n",
        "\n",
        "    Use this class by either passing a path to a torch file (.pt)\n",
        "    containing (data, targets) or pass the data, targets directly\n",
        "    instead.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_to_data=None,\n",
        "        data=None,\n",
        "        targets=None,\n",
        "        transform: Optional[Callable] = None,\n",
        "    ) -> None:\n",
        "        path = path_to_data.parent if path_to_data else None\n",
        "        super(TorchVision_FL, self).__init__(path, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "        if path_to_data:\n",
        "            # load data and targets (path_to_data points to an specific .pt file)\n",
        "            self.data, self.targets = torch.load(path_to_data)\n",
        "        else:\n",
        "            self.data = data\n",
        "            self.targets = targets\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        img, target = self.data[index], int(self.targets[index])\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        if not isinstance(img, Image.Image):  # if not PIL image\n",
        "            if not isinstance(img, np.ndarray):  # if torch tensor\n",
        "                img = img.numpy()\n",
        "\n",
        "            img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def getCIFAR10(path_to_data=\"./data\"):\n",
        "    \"\"\"Downloads CIFAR10 dataset and generates a unified training set (it will\n",
        "    be partitioned later using the LDA partitioning mechanism.\"\"\"\n",
        "\n",
        "    # download dataset and load train set\n",
        "    train_set = datasets.CIFAR10(root=path_to_data, train=True, download=True)\n",
        "\n",
        "    # fuse all data splits into a single \"training.pt\"\n",
        "    data_loc = Path(path_to_data) / \"cifar-10-batches-py\"\n",
        "    training_data = data_loc / \"training.pt\"\n",
        "    print(\"Generating unified CIFAR dataset\")\n",
        "    torch.save([train_set.data, np.array(train_set.targets)], training_data)\n",
        "\n",
        "    test_set = datasets.CIFAR10(\n",
        "        root=path_to_data, train=False, transform=cifar10Transformation()\n",
        "    )\n",
        "\n",
        "    # returns path where training data is and testset\n",
        "    return training_data, test_set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import flwr as fl\n",
        "from flwr.common.typing import Scalar\n",
        "import ray\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from typing import Dict, Callable, Optional, Tuple\n",
        "from dataset_utils import getCIFAR10, do_fl_partitioning, get_dataloader\n",
        "\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import os\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "# Model (simple CNN adapted from 'PyTorch: A 60 Minute Blitz')\n",
        "# borrowed from Pytorch quickstart example\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# borrowed from Pytorch quickstart example\n",
        "def train(net, trainloader, epochs, device: str):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "    net.train()\n",
        "\n",
        "    print(\"start profiling...\")\n",
        "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "        for _ in range(epochs):\n",
        "           for images, labels in trainloader:\n",
        "               images, labels = images.to(device), labels.to(device)\n",
        "               optimizer.zero_grad()\n",
        "               loss = criterion(net(images), labels)\n",
        "               loss.backward()\n",
        "               optimizer.step()\n",
        "    list1 = prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=0)\n",
        "    print(\"end profiling\")\n",
        "\n",
        "    import re\n",
        "    find_float = lambda x: re.search(\"\\d+(\\.\\d+)?s\", x).group()\n",
        "    cpu_time = float(find_float(str(list1))[:-1])\n",
        "    print(\"cpu_time\", cpu_time)\n",
        "    return cpu_time\n",
        "\n",
        "\n",
        "# borrowed from Pytorch quickstart example\n",
        "def test(net, testloader, device: str):\n",
        "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            # images, labels = data[0], data[1]\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "# Flower client that will be spawned by Ray\n",
        "# Adapted from Pytorch quickstart example\n",
        "class CifarRayClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid: str, fed_dir_data: str):\n",
        "        self.cid = cid\n",
        "        self.fed_dir = Path(fed_dir_data)\n",
        "        \n",
        "        if(os.path.exists(f\"client_properties_{self.cid}.pickle\")):\n",
        "            cpkl = open(f\"client_properties_{self.cid}.pickle\", 'rb')\n",
        "            data = pkl.load(cpkl)\n",
        "            self.properties: Dict[str, Scalar] = {\"tensor_type\": \"numpy.ndarray\", \"cpu_time\": data['cpu_time']}\n",
        "        else:\n",
        "            self.properties: Dict[str, Scalar] = {\"tensor_type\": \"numpy.ndarray\"}\n",
        "\n",
        "        # print(\"construction: self.properties\", self.properties)\n",
        "\n",
        "        # instantiate model\n",
        "        # self.net = Net()\n",
        "\n",
        "        # determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_parameters(self, net=None):\n",
        "        if(net == None):\n",
        "            net = Net()\n",
        "        return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "    # def get_properties(self, ins: PropertiesIns) -> PropertiesRes:\n",
        "    def get_properties(self, ins):\n",
        "        return self.properties\n",
        "    \n",
        "    def set_parameters(self, parameters):\n",
        "        net = Net()\n",
        "        params_dict = zip(net.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict(\n",
        "            {k: torch.from_numpy(np.copy(v)) for k, v in params_dict}\n",
        "        )\n",
        "        net.load_state_dict(state_dict, strict=True)\n",
        "        return net\n",
        "        \n",
        "    def fit(self, parameters, config):\n",
        "        \n",
        "        # print(f\"fit() on client cid={self.cid}\")\n",
        "        net = self.set_parameters(parameters)\n",
        "        \n",
        "        # load data for this client and get trainloader\n",
        "        num_workers = len(ray.worker.get_resource_ids()[\"CPU\"])\n",
        "        trainloader = get_dataloader(\n",
        "            self.fed_dir,\n",
        "            self.cid,\n",
        "            is_train=True,\n",
        "            batch_size=int(config[\"batch_size\"]),\n",
        "            workers=num_workers,\n",
        "        )\n",
        "        \n",
        "        # send model to device\n",
        "        net.to(self.device)\n",
        "        \n",
        "        # train\n",
        "        cpu_time = train(net, trainloader, epochs=int(config[\"epochs\"]), device=self.device)\n",
        "        self.properties['cpu_time'] = cpu_time\n",
        "        print(\"properties:\", self.properties)\n",
        "        \n",
        "        f = open(f\"client_properties_{self.cid}.pickle\",'wb')\n",
        "        pkl.dump(self.properties, f)\n",
        "        f.close()\n",
        "        \n",
        "        # return local model and statistics\n",
        "        return self.get_parameters(net), len(trainloader.dataset), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "\n",
        "        # print(f\"evaluate() on client cid={self.cid}\")\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # load data for this client and get trainloader\n",
        "        num_workers = len(ray.worker.get_resource_ids()[\"CPU\"])\n",
        "        valloader = get_dataloader(\n",
        "            self.fed_dir, self.cid, is_train=False, batch_size=50, workers=num_workers\n",
        "        )\n",
        "\n",
        "        # send model to device\n",
        "        self.net.to(self.device)\n",
        "\n",
        "        # evaluate\n",
        "        loss, accuracy = test(self.net, valloader, device=self.device)\n",
        "\n",
        "        # return statistics\n",
        "        return float(loss), len(valloader.dataset), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "\n",
        "def fit_config(rnd: int) -> Dict[str, str]:\n",
        "    \"\"\"Return a configuration with static batch size and (local) epochs.\"\"\"\n",
        "    config = {\n",
        "        \"epoch_global\": str(rnd),\n",
        "        \"epochs\": str(5),\n",
        "        \"batch_size\": str(64),\n",
        "    }\n",
        "    return config\n",
        "\n",
        "\n",
        "def set_weights(model: torch.nn.ModuleList, weights: fl.common.Weights) -> None:\n",
        "    \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
        "    state_dict = OrderedDict(\n",
        "        {\n",
        "            k: torch.tensor(np.atleast_1d(v))\n",
        "            for k, v in zip(model.state_dict().keys(), weights)\n",
        "        }\n",
        "    )\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def get_eval_fn(\n",
        "    testset: torchvision.datasets.CIFAR10,\n",
        ") -> Callable[[fl.common.Weights], Optional[Tuple[float, float]]]:\n",
        "    \"\"\"Return an evaluation function for centralized evaluation.\"\"\"\n",
        "\n",
        "    def evaluate(weights: fl.common.Weights) -> Optional[Tuple[float, float]]:\n",
        "        \"\"\"Use the entire CIFAR-10 test set for evaluation.\"\"\"\n",
        "\n",
        "        # determine device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        model = Net()\n",
        "        set_weights(model, weights)\n",
        "        model.to(device)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(testset, batch_size=50)\n",
        "        loss, accuracy = test(model, testloader, device=device)\n",
        "\n",
        "        # return statistics\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate\n",
        "\n",
        "\n",
        "# Start Ray simulation (a _default server_ will be created)\n",
        "# This example does:\n",
        "# 1. Downloads CIFAR-10\n",
        "# 2. Partitions the dataset into N splits, where N is the total number of\n",
        "#    clients. We refere to this as `pool_size`. The partition can be IID or non-IID\n",
        "# 4. Starts a Ray-based simulation where a % of clients are sample each round.\n",
        "# 5. After the M rounds end, the global model is evaluated on the entire testset.\n",
        "#    Also, the global model is evaluated on the valset partition residing in each\n",
        "#    client. This is useful to get a sense on how well the global model can generalise\n",
        "#    to each client's data.\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    pool_size = 100  # number of dataset partions (= number of total clients)\n",
        "    client_resources = {\"num_cpus\": 1}  # each client will get allocated 1 CPUs\n",
        "\n",
        "    # download CIFAR10 dataset\n",
        "    train_path, testset = getCIFAR10()\n",
        "\n",
        "    # partition dataset (use a large `alpha` to make it IID;\n",
        "    # a small value (e.g. 1) will make it non-IID)\n",
        "    # This will create a new directory called \"federated: in the directory where\n",
        "    # CIFAR-10 lives. Inside it, there will be N=pool_size sub-directories each with\n",
        "    # its own train/set split.\n",
        "    fed_dir = do_fl_partitioning(\n",
        "        train_path, pool_size=pool_size, alpha=1000, num_classes=10, val_ratio=0.1\n",
        "    )\n",
        "\n",
        "    # configure the strategy\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=0.1,\n",
        "        min_fit_clients=10,\n",
        "        min_available_clients=pool_size,  # All clients should be available\n",
        "        on_fit_config_fn=fit_config,\n",
        "        eval_fn=get_eval_fn(testset),  # centralised testset evaluation of global model\n",
        "    )\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        # create a single client instance\n",
        "        return CifarRayClient(cid, fed_dir)\n",
        "\n",
        "    # (optional) specify ray config\n",
        "    ray_config = {\"include_dashboard\": False}\n",
        "\n",
        "    # start simulation\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=pool_size,\n",
        "        client_resources=client_resources,\n",
        "        num_rounds=5,\n",
        "        strategy=strategy,\n",
        "        ray_init_args=ray_config,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrTy8HsW9V7k",
        "outputId": "2762a923-81f4-4a90-a4b5-ae43316ba18b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG0NhrcoZgOY",
        "outputId": "ea79a6b2-26a6-4c56-b11d-70e3dee01bb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "170499072it [00:04, 41266341.41it/s]                   \n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Generating unified CIFAR dataset\n",
            "Class histogram for 0-th partition (alpha=1000, 10 classes): [43 54 54 53 51 52 42 51 48 52]\n",
            "INFO flower 2022-02-02 15:24:19,123 | app.py:95 | Ray initialized with resources: {'memory': 6440226816.0, 'object_store_memory': 3220113408.0, 'CPU': 2.0, 'node:172.28.0.2': 1.0}\n",
            "INFO flower 2022-02-02 15:24:19,127 | app.py:104 | Starting Flower simulation running: {'num_rounds': 5}\n",
            "INFO flower 2022-02-02 15:24:19,131 | server.py:118 | Initializing global parameters\n",
            "INFO flower 2022-02-02 15:24:19,131 | server.py:304 | Requesting initial parameters from one random client\n",
            "INFO flower 2022-02-02 15:24:21,115 | server.py:307 | Received initial parameters from one random client\n",
            "INFO flower 2022-02-02 15:24:21,115 | server.py:120 | Evaluating initial parameters\n",
            "INFO flower 2022-02-02 15:24:24,418 | server.py:127 | initial parameters (loss, other metrics): 461.13446378707886, {'accuracy': 0.1}\n",
            "INFO flower 2022-02-02 15:24:24,418 | server.py:133 | FL starting\n",
            "DEBUG flower 2022-02-02 15:24:24,418 | server.py:255 | fit_round: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 1.992\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 1.992}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.012\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.012}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 1.969\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 1.969}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 1.91\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 1.91}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.072\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.072}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 1.999\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 1.999}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.204\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.204}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.032\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.032}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.226\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.226}\n",
            "DEBUG flower 2022-02-02 15:24:48,914 | server.py:264 | fit_round received 10 results and 0 failures\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.117\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.117}\n",
            "INFO flower 2022-02-02 15:24:52,014 | server.py:154 | fit progress: (1, 450.725750207901, {'accuracy': 0.1776}, 27.595451333000028)\n",
            "INFO flower 2022-02-02 15:24:52,014 | server.py:199 | evaluate_round: no clients selected, cancel\n",
            "DEBUG flower 2022-02-02 15:24:52,014 | server.py:255 | fit_round: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 3.165\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 3.165}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.792\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.792}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.45\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.45}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.51\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.51}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.5}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.39\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.39}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.459\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.459}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.497\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.497}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "DEBUG flower 2022-02-02 15:25:17,755 | server.py:264 | fit_round received 10 results and 0 failures\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.484\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.484}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.536\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.536}\n",
            "INFO flower 2022-02-02 15:25:20,864 | server.py:154 | fit progress: (2, 411.17773509025574, {'accuracy': 0.2499}, 56.44626048700002)\n",
            "INFO flower 2022-02-02 15:25:20,864 | server.py:199 | evaluate_round: no clients selected, cancel\n",
            "DEBUG flower 2022-02-02 15:25:20,865 | server.py:255 | fit_round: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.482\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.482}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.443\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.443}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.392\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.392}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.591\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.591}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.368\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.368}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.363\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.363}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.483\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.483}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.433\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.433}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.3\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.3}\n",
            "DEBUG flower 2022-02-02 15:25:45,608 | server.py:264 | fit_round received 10 results and 0 failures\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.511\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.511}\n",
            "INFO flower 2022-02-02 15:25:48,718 | server.py:154 | fit progress: (3, 397.05469846725464, {'accuracy': 0.2767}, 84.30042500000002)\n",
            "INFO flower 2022-02-02 15:25:48,719 | server.py:199 | evaluate_round: no clients selected, cancel\n",
            "DEBUG flower 2022-02-02 15:25:48,719 | server.py:255 | fit_round: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.565\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.565}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.43\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.43}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.413\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.413}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.502\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.502}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.464\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.464}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.549\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.549}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.439\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.439}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.488\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.488}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "DEBUG flower 2022-02-02 15:26:13,477 | server.py:264 | fit_round received 10 results and 0 failures\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.633\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.633}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.45\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.45}\n",
            "INFO flower 2022-02-02 15:26:16,589 | server.py:154 | fit progress: (4, 379.7610105276108, {'accuracy': 0.3194}, 112.17103614800004)\n",
            "INFO flower 2022-02-02 15:26:16,589 | server.py:199 | evaluate_round: no clients selected, cancel\n",
            "DEBUG flower 2022-02-02 15:26:16,589 | server.py:255 | fit_round: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.426\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.426}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.642\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.642}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.383\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.383}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.534\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.534}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.481\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.481}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.659\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.659}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.354\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.354}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.494\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.494}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m start profiling...\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m cpu_time 2.608\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m end profiling\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m cpu_time 2.5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=545)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.5}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=543)\u001b[0m properties: {'tensor_type': 'numpy.ndarray', 'cpu_time': 2.608}\n",
            "DEBUG flower 2022-02-02 15:26:41,547 | server.py:264 | fit_round received 10 results and 0 failures\n",
            "INFO flower 2022-02-02 15:26:44,625 | server.py:154 | fit progress: (5, 374.1025859117508, {'accuracy': 0.3248}, 140.206931656)\n",
            "INFO flower 2022-02-02 15:26:44,625 | server.py:199 | evaluate_round: no clients selected, cancel\n",
            "INFO flower 2022-02-02 15:26:44,625 | server.py:172 | FL finished in 140.20715233200002\n",
            "INFO flower 2022-02-02 15:26:44,626 | app.py:119 | app_fit: losses_distributed []\n",
            "INFO flower 2022-02-02 15:26:44,626 | app.py:120 | app_fit: metrics_distributed {}\n",
            "INFO flower 2022-02-02 15:26:44,626 | app.py:121 | app_fit: losses_centralized [(0, 461.13446378707886), (1, 450.725750207901), (2, 411.17773509025574), (3, 397.05469846725464), (4, 379.7610105276108), (5, 374.1025859117508)]\n",
            "INFO flower 2022-02-02 15:26:44,626 | app.py:122 | app_fit: metrics_centralized {'accuracy': [(0, 0.1), (1, 0.1776), (2, 0.2499), (3, 0.2767), (4, 0.3194), (5, 0.3248)]}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import flwr as fl\n",
        "from flwr.common.typing import Scalar\n",
        "import ray\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from typing import Dict, Callable, Optional, Tuple\n",
        "from dataset_utils import getCIFAR10, do_fl_partitioning, get_dataloader\n",
        "\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import os\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "# Model (simple CNN adapted from 'PyTorch: A 60 Minute Blitz')\n",
        "# borrowed from Pytorch quickstart example\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# borrowed from Pytorch quickstart example\n",
        "def train(net, trainloader, epochs, device: str):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "    net.train()\n",
        "\n",
        "    print(\"start profiling...\")\n",
        "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "        for _ in range(epochs):\n",
        "           for images, labels in trainloader:\n",
        "               images, labels = images.to(device), labels.to(device)\n",
        "               optimizer.zero_grad()\n",
        "               loss = criterion(net(images), labels)\n",
        "               loss.backward()\n",
        "               optimizer.step()\n",
        "    # print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n",
        "    # os.chdir(\"/\")\n",
        "    # f = open(\"profiling.txt\",\"w\")\n",
        "    list1 = prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=0)\n",
        "    # print(\"list1 = \", list1)\n",
        "    print(\"end profiling\")\n",
        "\n",
        "    # with open('profiling.txt', 'r') as f1:\n",
        "    #    list1 = f1.readlines()\n",
        "    # print(\"result:\", list1[(len(list1)-2)])\n",
        "    import re\n",
        "    # print(\"extract:\", list1[(len(list1)-2)])\n",
        "    # find_float = lambda x: re.search(\"\\d+(\\.\\d+)?s\", x).group()\n",
        "    # cpu_time = float(find_float(str(list1)))\n",
        "    find_float = lambda x: re.search(\"\\d+(\\.\\d+)?s\", x).group()\n",
        "    cpu_time = float(find_float(str(list1))[:-1])\n",
        "    # cpu_time = float(find_float(str(list1[(len(list1)-2)])))\n",
        "    print(\"cpu_time\", cpu_time)\n",
        "    return cpu_time\n",
        "\n",
        "\n",
        "# borrowed from Pytorch quickstart example\n",
        "def test(net, testloader, device: str):\n",
        "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            # images, labels = data[0], data[1]\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "# Flower client that will be spawned by Ray\n",
        "# Adapted from Pytorch quickstart example\n",
        "class CifarRayClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid: str, fed_dir_data: str):\n",
        "        self.cid = cid\n",
        "        self.fed_dir = Path(fed_dir_data)\n",
        "        self.properties: Dict[str, Scalar] = {\"tensor_type\": \"numpy.ndarray\"}\n",
        "\n",
        "        # instantiate model\n",
        "        self.net = Net()\n",
        "\n",
        "        # determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
        "\n",
        "    # def get_properties(self, ins: PropertiesIns) -> PropertiesRes:\n",
        "    def get_properties(self, ins):\n",
        "        return self.properties\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        params_dict = zip(self.net.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict(\n",
        "            {k: torch.from_numpy(np.copy(v)) for k, v in params_dict}\n",
        "        )\n",
        "        self.net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "\n",
        "        # print(f\"fit() on client cid={self.cid}\")\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # load data for this client and get trainloader\n",
        "        num_workers = len(ray.worker.get_resource_ids()[\"CPU\"])\n",
        "        trainloader = get_dataloader(\n",
        "            self.fed_dir,\n",
        "            self.cid,\n",
        "            is_train=True,\n",
        "            batch_size=int(config[\"batch_size\"]),\n",
        "            workers=num_workers,\n",
        "        )\n",
        "\n",
        "        # send model to device\n",
        "        self.net.to(self.device)\n",
        "\n",
        "        # train\n",
        "        cpu_time = train(self.net, trainloader, epochs=int(config[\"epochs\"]), device=self.device)\n",
        "        self.properties['cpu_time'] = cpu_time\n",
        "        print(\"properties:\", self.properties)\n",
        "\n",
        "        f = open(f\"client_properties_{self.cid}.pickle\",'wb')\n",
        "        pkl.dump(self.properties, f)\n",
        "        f.close()\n",
        "\n",
        "        # return local model and statistics\n",
        "        return self.get_parameters(), len(trainloader.dataset), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "\n",
        "        # print(f\"evaluate() on client cid={self.cid}\")\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # load data for this client and get trainloader\n",
        "        num_workers = len(ray.worker.get_resource_ids()[\"CPU\"])\n",
        "        valloader = get_dataloader(\n",
        "            self.fed_dir, self.cid, is_train=False, batch_size=50, workers=num_workers\n",
        "        )\n",
        "\n",
        "        # send model to device\n",
        "        self.net.to(self.device)\n",
        "\n",
        "        # evaluate\n",
        "        loss, accuracy = test(self.net, valloader, device=self.device)\n",
        "\n",
        "        # return statistics\n",
        "        return float(loss), len(valloader.dataset), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "\n",
        "def fit_config(rnd: int) -> Dict[str, str]:\n",
        "    \"\"\"Return a configuration with static batch size and (local) epochs.\"\"\"\n",
        "    config = {\n",
        "        \"epoch_global\": str(rnd),\n",
        "        \"epochs\": str(5),\n",
        "        \"batch_size\": str(64),\n",
        "    }\n",
        "    return config\n",
        "\n",
        "\n",
        "def set_weights(model: torch.nn.ModuleList, weights: fl.common.Weights) -> None:\n",
        "    \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
        "    state_dict = OrderedDict(\n",
        "        {\n",
        "            k: torch.tensor(np.atleast_1d(v))\n",
        "            for k, v in zip(model.state_dict().keys(), weights)\n",
        "        }\n",
        "    )\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def get_eval_fn(\n",
        "    testset: torchvision.datasets.CIFAR10,\n",
        ") -> Callable[[fl.common.Weights], Optional[Tuple[float, float]]]:\n",
        "    \"\"\"Return an evaluation function for centralized evaluation.\"\"\"\n",
        "\n",
        "    def evaluate(weights: fl.common.Weights) -> Optional[Tuple[float, float]]:\n",
        "        \"\"\"Use the entire CIFAR-10 test set for evaluation.\"\"\"\n",
        "\n",
        "        # determine device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        model = Net()\n",
        "        set_weights(model, weights)\n",
        "        model.to(device)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(testset, batch_size=50)\n",
        "        loss, accuracy = test(model, testloader, device=device)\n",
        "\n",
        "        # return statistics\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate\n",
        "\n",
        "\n",
        "# Start Ray simulation (a _default server_ will be created)\n",
        "# This example does:\n",
        "# 1. Downloads CIFAR-10\n",
        "# 2. Partitions the dataset into N splits, where N is the total number of\n",
        "#    clients. We refere to this as `pool_size`. The partition can be IID or non-IID\n",
        "# 4. Starts a Ray-based simulation where a % of clients are sample each round.\n",
        "# 5. After the M rounds end, the global model is evaluated on the entire testset.\n",
        "#    Also, the global model is evaluated on the valset partition residing in each\n",
        "#    client. This is useful to get a sense on how well the global model can generalise\n",
        "#    to each client's data.\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    pool_size = 100  # number of dataset partions (= number of total clients)\n",
        "    client_resources = {\"num_cpus\": 1}  # each client will get allocated 1 CPUs\n",
        "\n",
        "    # download CIFAR10 dataset\n",
        "    train_path, testset = getCIFAR10()\n",
        "\n",
        "    # partition dataset (use a large `alpha` to make it IID;\n",
        "    # a small value (e.g. 1) will make it non-IID)\n",
        "    # This will create a new directory called \"federated: in the directory where\n",
        "    # CIFAR-10 lives. Inside it, there will be N=pool_size sub-directories each with\n",
        "    # its own train/set split.\n",
        "    fed_dir = do_fl_partitioning(\n",
        "        train_path, pool_size=pool_size, alpha=1000, num_classes=10, val_ratio=0.1\n",
        "    )\n",
        "\n",
        "    # configure the strategy\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=0.1,\n",
        "        min_fit_clients=10,\n",
        "        min_available_clients=pool_size,  # All clients should be available\n",
        "        on_fit_config_fn=fit_config,\n",
        "        eval_fn=get_eval_fn(testset),  # centralised testset evaluation of global model\n",
        "    )\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        # create a single client instance\n",
        "        return CifarRayClient(cid, fed_dir)\n",
        "\n",
        "    # (optional) specify ray config\n",
        "    ray_config = {\"include_dashboard\": False}\n",
        "\n",
        "    # start simulation\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=pool_size,\n",
        "        client_resources=client_resources,\n",
        "        num_rounds=5,\n",
        "        strategy=strategy,\n",
        "        ray_init_args=ray_config,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH5uVhbqZWbh",
        "outputId": "06fc100e-f4f8-4240-892f-393128b92f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import flwr as fl\n",
        "from flwr.common.typing import Scalar\n",
        "import ray\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from typing import Dict, Callable, Optional, Tuple\n",
        "from dataset_utils import getCIFAR10, do_fl_partitioning, get_dataloader\n",
        "\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "\n",
        "# Model (simple CNN adapted from 'PyTorch: A 60 Minute Blitz')\n",
        "# borrowed from Pytorch quickstart example\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# borrowed from Pytorch quickstart example\n",
        "def train(net, trainloader, epochs, device: str):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "    net.train()\n",
        "    for _ in range(epochs):\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(net(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "# borrowed from Pytorch quickstart example\n",
        "def test(net, testloader, device: str):\n",
        "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            # images, labels = data[0], data[1]\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "# Flower client that will be spawned by Ray\n",
        "# Adapted from Pytorch quickstart example\n",
        "class CifarRayClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid: str, fed_dir_data: str):\n",
        "        self.cid = cid\n",
        "        self.fed_dir = Path(fed_dir_data)\n",
        "        self.properties: Dict[str, Scalar] = {\"tensor_type\": \"numpy.ndarray\"}\n",
        "\n",
        "        # instantiate model\n",
        "        self.net = Net()\n",
        "\n",
        "        # determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
        "\n",
        "    # def get_properties(self, ins: PropertiesIns) -> PropertiesRes:\n",
        "    def get_properties(self, ins):\n",
        "        return self.properties\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        params_dict = zip(self.net.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict(\n",
        "            {k: torch.from_numpy(np.copy(v)) for k, v in params_dict}\n",
        "        )\n",
        "        self.net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "\n",
        "        # print(f\"fit() on client cid={self.cid}\")\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # load data for this client and get trainloader\n",
        "        num_workers = len(ray.worker.get_resource_ids()[\"CPU\"])\n",
        "        trainloader = get_dataloader(\n",
        "            self.fed_dir,\n",
        "            self.cid,\n",
        "            is_train=True,\n",
        "            batch_size=int(config[\"batch_size\"]),\n",
        "            workers=num_workers,\n",
        "        )\n",
        "\n",
        "        # send model to device\n",
        "        self.net.to(self.device)\n",
        "\n",
        "        # train\n",
        "        train(self.net, trainloader, epochs=int(config[\"epochs\"]), device=self.device)\n",
        "\n",
        "        # profiling\n",
        "        print(\"start profiling...\")\n",
        "        with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "            print(\"start enumerate...\")\n",
        "            # print(\"enumerate(trainloader):\", list(enumerate(trainloader)))\n",
        "            # for ii, data in enumerate(trainloader):\n",
        "            #    print(\"ii:\", ii)\n",
        "            #    if ii > 10:\n",
        "            #        print(\"break out\")\n",
        "            #        break\n",
        "            #    print(\"here 0\")\n",
        "                # images, labels = data[0].to(device), data[1].to(device)\n",
        "                # print(data[0], data[1])\n",
        "                # images = data[0]\n",
        "            #    print(data[1])\n",
        "                # labels = data[1]\n",
        "            #    print(\"here 1\")\n",
        "                # optimizer.zero_grad()\n",
        "            #    print(\"here 2\")\n",
        "                # loss = criterion(net(images), labels)\n",
        "            #    print(\"here 3\")\n",
        "                # loss.backward()\n",
        "            #    print(\"here 4\")\n",
        "                # optimizer.step()\n",
        "            #    print(\"here 5\")\n",
        "            with record_function(\"train\"):\n",
        "                print(\"before\")\n",
        "                self.net(trainloader)\n",
        "                print(\"after\")\n",
        "                # prof.step()\n",
        "                # print(\"ii end:\", ii)\n",
        "        # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
        "        print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n",
        "        print(\"end profiling\")\n",
        "\n",
        "        # return local model and statistics\n",
        "        return self.get_parameters(), len(trainloader.dataset), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "\n",
        "        # print(f\"evaluate() on client cid={self.cid}\")\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # load data for this client and get trainloader\n",
        "        num_workers = len(ray.worker.get_resource_ids()[\"CPU\"])\n",
        "        valloader = get_dataloader(\n",
        "            self.fed_dir, self.cid, is_train=False, batch_size=50, workers=num_workers\n",
        "        )\n",
        "\n",
        "        # send model to device\n",
        "        self.net.to(self.device)\n",
        "\n",
        "        # evaluate\n",
        "        loss, accuracy = test(self.net, valloader, device=self.device)\n",
        "\n",
        "        # return statistics\n",
        "        return float(loss), len(valloader.dataset), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "\n",
        "def fit_config(rnd: int) -> Dict[str, str]:\n",
        "    \"\"\"Return a configuration with static batch size and (local) epochs.\"\"\"\n",
        "    config = {\n",
        "        \"epoch_global\": str(rnd),\n",
        "        \"epochs\": str(5),\n",
        "        \"batch_size\": str(64),\n",
        "    }\n",
        "    return config\n",
        "\n",
        "\n",
        "def set_weights(model: torch.nn.ModuleList, weights: fl.common.Weights) -> None:\n",
        "    \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
        "    state_dict = OrderedDict(\n",
        "        {\n",
        "            k: torch.tensor(np.atleast_1d(v))\n",
        "            for k, v in zip(model.state_dict().keys(), weights)\n",
        "        }\n",
        "    )\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def get_eval_fn(\n",
        "    testset: torchvision.datasets.CIFAR10,\n",
        ") -> Callable[[fl.common.Weights], Optional[Tuple[float, float]]]:\n",
        "    \"\"\"Return an evaluation function for centralized evaluation.\"\"\"\n",
        "\n",
        "    def evaluate(weights: fl.common.Weights) -> Optional[Tuple[float, float]]:\n",
        "        \"\"\"Use the entire CIFAR-10 test set for evaluation.\"\"\"\n",
        "\n",
        "        # determine device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        model = Net()\n",
        "        set_weights(model, weights)\n",
        "        model.to(device)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(testset, batch_size=50)\n",
        "        loss, accuracy = test(model, testloader, device=device)\n",
        "\n",
        "        # return statistics\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate\n",
        "\n",
        "\n",
        "# Start Ray simulation (a _default server_ will be created)\n",
        "# This example does:\n",
        "# 1. Downloads CIFAR-10\n",
        "# 2. Partitions the dataset into N splits, where N is the total number of\n",
        "#    clients. We refere to this as `pool_size`. The partition can be IID or non-IID\n",
        "# 4. Starts a Ray-based simulation where a % of clients are sample each round.\n",
        "# 5. After the M rounds end, the global model is evaluated on the entire testset.\n",
        "#    Also, the global model is evaluated on the valset partition residing in each\n",
        "#    client. This is useful to get a sense on how well the global model can generalise\n",
        "#    to each client's data.\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    pool_size = 100  # number of dataset partions (= number of total clients)\n",
        "    client_resources = {\"num_cpus\": 1}  # each client will get allocated 1 CPUs\n",
        "\n",
        "    # download CIFAR10 dataset\n",
        "    train_path, testset = getCIFAR10()\n",
        "\n",
        "    # partition dataset (use a large `alpha` to make it IID;\n",
        "    # a small value (e.g. 1) will make it non-IID)\n",
        "    # This will create a new directory called \"federated: in the directory where\n",
        "    # CIFAR-10 lives. Inside it, there will be N=pool_size sub-directories each with\n",
        "    # its own train/set split.\n",
        "    fed_dir = do_fl_partitioning(\n",
        "        train_path, pool_size=pool_size, alpha=1000, num_classes=10, val_ratio=0.1\n",
        "    )\n",
        "\n",
        "    # configure the strategy\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=0.1,\n",
        "        min_fit_clients=10,\n",
        "        min_available_clients=pool_size,  # All clients should be available\n",
        "        on_fit_config_fn=fit_config,\n",
        "        eval_fn=get_eval_fn(testset),  # centralised testset evaluation of global model\n",
        "    )\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        # create a single client instance\n",
        "        return CifarRayClient(cid, fed_dir)\n",
        "\n",
        "    # (optional) specify ray config\n",
        "    ray_config = {\"include_dashboard\": False}\n",
        "\n",
        "    # start simulation\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=pool_size,\n",
        "        client_resources=client_resources,\n",
        "        num_rounds=5,\n",
        "        strategy=strategy,\n",
        "        ray_init_args=ray_config,\n",
        "    )"
      ],
      "metadata": {
        "id": "nW2avmoSpTtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    import re\n",
        "    list1 = \"Total CPU time: 1.23s\"\n",
        "    # print(\"extract:\", list1[(len(list1)-2)])\n",
        "    find_float = lambda x: re.search(\"\\d+(\\.\\d+)?s\", x).group()\n",
        "    cpu_time = float(find_float(str(list1))[:-1])\n",
        "    # cpu_time = float(find_float(str(list1[(len(list1)-2)])))\n",
        "    print(\"cpu_time\", cpu_time)"
      ],
      "metadata": {
        "id": "8hNdDbeB4Ubh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123640d8-f49b-4e71-992b-308b98402d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu_time 1.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M57-8T8gPLzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}